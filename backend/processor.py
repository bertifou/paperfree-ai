import os
import json
import base64
import logging
import pytesseract
from PIL import Image
import PyPDF2
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# Valeurs par dÃ©faut lues depuis .env, avec fallback sur la DB si besoin
DEFAULT_LLM_CONFIG = {
    "base_url": os.getenv("LLM_BASE_URL", "http://localhost:1234/v1"),
    "api_key":  os.getenv("LLM_API_KEY",  "lm-studio"),
    "model":    os.getenv("LLM_MODEL",    "local-model"),
}

# Backends connus â€” URL de base par dÃ©faut (utile pour l'UI et la validation)
KNOWN_BACKENDS = {
    "lm_studio": "http://localhost:1234/v1",
    "ollama":    "http://localhost:11434/v1",
    "openai":    "https://api.openai.com/v1",
    "gemini":    "https://generativelanguage.googleapis.com/v1beta/openai/",
}

# ModÃ¨les Gemini recommandÃ©s (exposÃ©s via /backends pour l'UI)
# Note : "gemini-3" n'existe pas encore â€” le dernier preview est gemini-2.5-flash
GEMINI_MODELS = [
    "gemini-2.5-flash-preview-05-20",
    "gemini-2.5-pro-preview-05-06",
    "gemini-2.0-flash",
    "gemini-1.5-flash",
    "gemini-1.5-pro",
]

SYSTEM_PROMPT = """Tu es un assistant spÃ©cialisÃ© dans l'analyse de documents administratifs.
Analyse le texte fourni et rÃ©ponds UNIQUEMENT avec un objet JSON valide contenant :
{
  "category": "une catÃ©gorie parmi : Facture, ImpÃ´ts, SantÃ©, Banque, Contrat, Assurance, Travail, Courrier, Autre",
  "summary": "rÃ©sumÃ© en 15 mots maximum",
  "date": "date principale du document au format YYYY-MM-DD ou null",
  "amount": "montant principal en chiffres avec devise ou null",
  "issuer": "organisme ou entreprise Ã©mettrice ou null"
}
Ne rÃ©ponds rien d'autre que le JSON."""

OCR_CORRECTION_PROMPT = """Tu es un expert en correction de texte OCR pour des documents administratifs franÃ§ais.
Le texte suivant a Ã©tÃ© extrait par OCR (reconnaissance optique de caractÃ¨res) et peut contenir des erreurs typiques :
- Lettres confondues (l/1/I, 0/O, rn/m, etc.)
- Espaces manquants ou en trop
- Ponctuation incorrecte
- Mots coupÃ©s

Corrige ces erreurs en te basant sur le contexte (document administratif franÃ§ais).
Retourne UNIQUEMENT le texte corrigÃ©, sans commentaires ni explications.
Conserve la structure et la mise en page originale autant que possible.
Score de confiance OCR fourni : {confidence}% â€” plus il est bas, plus la correction est importante."""

OCR_VISION_CORRECTION_PROMPT = """Tu es un expert en correction de texte OCR pour des documents administratifs franÃ§ais.
L'image originale du document t'est fournie ainsi que le texte extrait automatiquement par OCR.

Score de confiance OCR : {confidence}% â€” plus il est bas, plus les erreurs sont probables.

Erreurs OCR typiques Ã  corriger en t'aidant de l'image :
- Lettres confondues (l/1/I, 0/O, rn/m, cl/d, etc.)
- Espaces manquants ou en trop
- Ponctuation incorrecte
- Mots coupÃ©s ou fusionnÃ©s
- Chiffres mal reconnus dans les montants et dates

Texte OCR Ã  corriger :
{ocr_text}

Retourne UNIQUEMENT le texte corrigÃ©, sans commentaires ni explications.
Conserve la structure et la mise en page originale."""

VISION_SYSTEM_PROMPT = """Tu es un assistant spÃ©cialisÃ© dans l'analyse de documents administratifs par vision.
On te fournit l'image d'un document. Analyse-la et rÃ©ponds UNIQUEMENT avec un objet JSON valide contenant :
{
  "category": "une catÃ©gorie parmi : Facture, ImpÃ´ts, SantÃ©, Banque, Contrat, Assurance, Travail, Courrier, Autre",
  "summary": "rÃ©sumÃ© en 15 mots maximum",
  "date": "date principale du document au format YYYY-MM-DD ou null",
  "amount": "montant principal en chiffres avec devise ou null",
  "issuer": "organisme ou entreprise Ã©mettrice ou null",
  "extracted_text": "texte principal extrait du document (500 mots max)"
}
Ne rÃ©ponds rien d'autre que le JSON."""

def get_llm_config() -> dict:
    """Lit la config LLM depuis la DB, avec fallback sur les variables d'env."""
    try:
        from database import SessionLocal, Setting
        db = SessionLocal()
        settings = {s.key: s.value for s in db.query(Setting).all()}
        db.close()
        return {
            "base_url":        settings.get("llm_base_url")        or DEFAULT_LLM_CONFIG["base_url"],
            "api_key":         settings.get("llm_api_key")         or DEFAULT_LLM_CONFIG["api_key"],
            "model":           settings.get("llm_model")           or DEFAULT_LLM_CONFIG["model"],
            # Vision
            "vision_enabled":  settings.get("llm_vision_enabled",  "false").lower() == "true",
            "vision_provider": settings.get("llm_vision_provider", "local"),   # local | openai | anthropic
            "vision_model":    settings.get("llm_vision_model",    ""),        # vide = utiliser model principal
            "vision_api_key":  settings.get("llm_vision_api_key",  ""),
            "vision_base_url": settings.get("llm_vision_base_url", ""),
            # OCR
            "ocr_llm_correction": settings.get("ocr_llm_correction", "true").lower() == "true",
            "ocr_correction_threshold": int(settings.get("ocr_correction_threshold", "80")),
        }
    except Exception:
        return {**DEFAULT_LLM_CONFIG,
                "vision_enabled": False, "vision_provider": "local",
                "vision_model": "", "vision_api_key": "", "vision_base_url": "",
                "ocr_llm_correction": True, "ocr_correction_threshold": 80}


# ---------------------------------------------------------------------------
# GÃ©nÃ©ration PDF typographique (texte OCR â†’ vrai PDF mise en page)
# ---------------------------------------------------------------------------

def generate_text_pdf(
    text: str,
    output_dir: str,
    base_name: str,
    meta: dict | None = None,
) -> str | None:
    """
    GÃ©nÃ¨re un PDF propre et lisible Ã  partir du texte extrait (OCR/LLM).
    Utilise ReportLab pour une vraie mise en page typographique.
    meta peut contenir : category, summary, date, amount, issuer.
    Retourne le chemin du PDF gÃ©nÃ©rÃ©, ou None en cas d'erreur.
    """
    try:
        from reportlab.lib.pagesizes import A4
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.units import cm
        from reportlab.lib import colors
        from reportlab.platypus import (
            SimpleDocTemplate, Paragraph, Spacer, HRFlowable, Table, TableStyle
        )
        from reportlab.lib.enums import TA_LEFT, TA_CENTER
        import datetime as dt

        pdf_name = base_name + "_ocr.pdf"
        pdf_path = os.path.join(output_dir, pdf_name)
        meta = meta or {}

        doc = SimpleDocTemplate(
            pdf_path,
            pagesize=A4,
            leftMargin=2.5 * cm,
            rightMargin=2.5 * cm,
            topMargin=2.5 * cm,
            bottomMargin=2.5 * cm,
        )

        styles = getSampleStyleSheet()

        # â”€â”€ Styles personnalisÃ©s â”€â”€
        title_style = ParagraphStyle(
            "DocTitle",
            parent=styles["Heading1"],
            fontSize=16,
            textColor=colors.HexColor("#1e3a5f"),
            spaceAfter=4,
        )
        meta_label_style = ParagraphStyle(
            "MetaLabel",
            parent=styles["Normal"],
            fontSize=8,
            textColor=colors.HexColor("#6b7280"),
            spaceBefore=0,
            spaceAfter=0,
        )
        meta_value_style = ParagraphStyle(
            "MetaValue",
            parent=styles["Normal"],
            fontSize=9,
            textColor=colors.HexColor("#111827"),
            fontName="Helvetica-Bold",
            spaceBefore=0,
            spaceAfter=0,
        )
        body_style = ParagraphStyle(
            "Body",
            parent=styles["Normal"],
            fontSize=10,
            leading=15,
            textColor=colors.HexColor("#1f2937"),
            spaceAfter=6,
        )
        footer_style = ParagraphStyle(
            "Footer",
            parent=styles["Normal"],
            fontSize=7,
            textColor=colors.HexColor("#9ca3af"),
            alignment=TA_CENTER,
        )

        story = []

        # â”€â”€ En-tÃªte â”€â”€
        category = meta.get("category") or "Document"
        story.append(Paragraph(f"ðŸ“„ {category}", title_style))

        summary = meta.get("summary") or ""
        if summary:
            story.append(Paragraph(summary, ParagraphStyle(
                "Summary", parent=styles["Normal"],
                fontSize=10, textColor=colors.HexColor("#4b5563"), spaceAfter=10,
                fontName="Helvetica-Oblique",
            )))

        # â”€â”€ Bande de mÃ©tadonnÃ©es â”€â”€
        meta_fields = []
        if meta.get("date"):
            meta_fields.append([
                Paragraph("DATE", meta_label_style),
                Paragraph(meta["date"], meta_value_style),
            ])
        if meta.get("issuer"):
            meta_fields.append([
                Paragraph("Ã‰METTEUR", meta_label_style),
                Paragraph(meta["issuer"], meta_value_style),
            ])
        if meta.get("amount"):
            meta_fields.append([
                Paragraph("MONTANT", meta_label_style),
                Paragraph(meta["amount"], meta_value_style),
            ])

        if meta_fields:
            col_w = (A4[0] - 5 * cm) / len(meta_fields)
            tbl = Table(
                [
                    [f[0] for f in meta_fields],
                    [f[1] for f in meta_fields],
                ],
                colWidths=[col_w] * len(meta_fields),
            )
            tbl.setStyle(TableStyle([
                ("BACKGROUND",  (0, 0), (-1, -1), colors.HexColor("#f3f4f6")),
                ("ROUNDEDCORNERS", [6]),
                ("TOPPADDING",  (0, 0), (-1, -1), 6),
                ("BOTTOMPADDING", (0, 0), (-1, -1), 6),
                ("LEFTPADDING",  (0, 0), (-1, -1), 10),
                ("RIGHTPADDING", (0, 0), (-1, -1), 10),
                ("LINEBELOW",   (0, 0), (-1, 0), 0.5, colors.HexColor("#e5e7eb")),
            ]))
            story.append(tbl)
            story.append(Spacer(1, 0.5 * cm))

        story.append(HRFlowable(width="100%", thickness=1, color=colors.HexColor("#e5e7eb")))
        story.append(Spacer(1, 0.4 * cm))

        # â”€â”€ Corps du texte â”€â”€
        if text and text.strip():
            for line in text.splitlines():
                line = line.strip()
                if not line:
                    story.append(Spacer(1, 0.2 * cm))
                else:
                    # Ã‰chapper les caractÃ¨res spÃ©ciaux XML/HTML pour ReportLab
                    safe = (line
                            .replace("&", "&amp;")
                            .replace("<", "&lt;")
                            .replace(">", "&gt;"))
                    story.append(Paragraph(safe, body_style))
        else:
            story.append(Paragraph(
                "<i>Aucun texte extrait pour ce document.</i>",
                ParagraphStyle("Empty", parent=styles["Normal"],
                               fontSize=10, textColor=colors.HexColor("#9ca3af")),
            ))

        # â”€â”€ Pied de page â”€â”€
        story.append(Spacer(1, 0.6 * cm))
        story.append(HRFlowable(width="100%", thickness=0.5, color=colors.HexColor("#e5e7eb")))
        story.append(Spacer(1, 0.2 * cm))
        generated = dt.datetime.now().strftime("%Y-%m-%d %H:%M")
        story.append(Paragraph(
            f"Document gÃ©nÃ©rÃ© par PaperFree-AI Â· {generated}",
            footer_style,
        ))

        doc.build(story)
        logger.info(f"[pdf-gen] PDF typographique gÃ©nÃ©rÃ© : {pdf_path}")
        return pdf_path

    except Exception as e:
        logger.error(f"[pdf-gen] Erreur ReportLab : {e}")
        return None


# ---------------------------------------------------------------------------
# OCR avec score de confiance
# ---------------------------------------------------------------------------

def extract_text_with_confidence(file_path: str) -> tuple[str, float]:
    """
    Extrait le texte d'une image avec le score de confiance moyen Tesseract.
    Retourne (texte, confidence_0_to_100).
    Pour les PDF, retourne (texte, 100.0) car extraction native = fiable.
    """
    if file_path.lower().endswith(".pdf"):
        text = _extract_pdf_text(file_path)
        return text, 100.0

    try:
        img = Image.open(file_path)
        # DonnÃ©es dÃ©taillÃ©es incluant les scores par mot
        data = pytesseract.image_to_data(img, lang="fra+eng", output_type=pytesseract.Output.DICT)
        confidences = [int(c) for c in data["conf"] if int(c) >= 0]
        avg_conf = sum(confidences) / len(confidences) if confidences else 0.0
        # Texte brut standard
        text = pytesseract.image_to_string(img, lang="fra+eng").strip()
        logger.info(f"[ocr] Confiance moyenne : {avg_conf:.1f}% ({len(confidences)} mots)")
        return text, avg_conf
    except Exception as e:
        logger.error(f"[ocr] Erreur Tesseract : {e}")
        return "", 0.0

def _extract_pdf_text(file_path: str) -> str:
    """Extraction texte native depuis un PDF."""
    text = ""
    with open(file_path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text.strip()


# ---------------------------------------------------------------------------
# Correction OCR par LLM (texte uniquement)
# ---------------------------------------------------------------------------

def correct_ocr_with_llm(text: str, confidence: float, config: dict) -> str:
    """
    Soumet le texte OCR brut au LLM pour correction des erreurs typiques.
    UtilisÃ© quand la confiance est sous le seuil OU systÃ©matiquement selon config.
    """
    if not text.strip():
        return text

    threshold = config.get("ocr_correction_threshold", 80)
    if not config.get("ocr_llm_correction", True):
        return text  # Correction dÃ©sactivÃ©e dans les settings

    # Toujours corriger si confiance < seuil, sinon lÃ©gÃ¨re passe de nettoyage
    prompt = OCR_CORRECTION_PROMPT.format(confidence=f"{confidence:.0f}")
    logger.info(f"[ocr-correction] Confiance {confidence:.0f}% â†’ correction LLM activÃ©e")

    try:
        client = OpenAI(base_url=config["base_url"], api_key=config["api_key"])
        response = client.chat.completions.create(
            model=config["model"],
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user",   "content": text[:4000]},
            ],
            temperature=0.1,
        )
        corrected = response.choices[0].message.content.strip()
        logger.info(f"[ocr-correction] Texte corrigÃ© ({len(corrected)} chars)")
        return corrected
    except Exception as e:
        logger.warning(f"[ocr-correction] Erreur LLM, texte brut conservÃ© : {e}")
        return text


def correct_ocr_with_vision(file_path: str, ocr_text: str, confidence: float, config: dict) -> str:
    """
    Correction OCR avancÃ©e : passe l'image + le texte OCR + le score de confiance
    au LLM vision pour corriger les erreurs en se basant sur le document original.
    UtilisÃ© uniquement si vision activÃ©e et fichier image disponible.
    """
    if not ocr_text.strip():
        return ocr_text
    if not config.get("ocr_llm_correction", True):
        return ocr_text

    logger.info(f"[ocr-vision-correction] Confiance {confidence:.0f}% â†’ correction vision activÃ©e")
    try:
        mime, b64 = _image_to_base64(file_path)
        client, model = _get_vision_client(config)

        prompt = OCR_VISION_CORRECTION_PROMPT.format(
            confidence=f"{confidence:.0f}",
            ocr_text=ocr_text[:3000],
        )
        response = client.chat.completions.create(
            model=model,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "image_url",
                     "image_url": {"url": f"data:{mime};base64,{b64}", "detail": "high"}},
                    {"type": "text", "text": prompt},
                ],
            }],
            temperature=0.1,
            max_tokens=2000,
        )
        corrected = response.choices[0].message.content.strip()
        logger.info(f"[ocr-vision-correction] Texte corrigÃ© ({len(corrected)} chars)")
        return corrected
    except Exception as e:
        logger.warning(f"[ocr-vision-correction] Erreur, fallback correction texte : {e}")
        return correct_ocr_with_llm(ocr_text, confidence, config)


# ---------------------------------------------------------------------------
# Analyse par vision (image â†’ LLM multimodal)
# ---------------------------------------------------------------------------

def _get_vision_client(config: dict) -> tuple:
    """Retourne (client, model) selon le provider vision configurÃ©."""
    provider = config.get("vision_provider", "local")
    v_model  = config.get("vision_model") or config.get("model", "")
    v_key    = config.get("vision_api_key") or config.get("api_key", "")
    v_url    = config.get("vision_base_url") or config.get("base_url", "")

    if provider == "openai":
        client = OpenAI(api_key=v_key or os.getenv("OPENAI_API_KEY", ""))
        model  = v_model or "gpt-4o"
    elif provider == "anthropic":
        # Utilise l'API Anthropic via interface compatible OpenAI (claude-3-5-sonnet)
        client = OpenAI(
            base_url="https://api.anthropic.com/v1",
            api_key=v_key or os.getenv("ANTHROPIC_API_KEY", ""),
        )
        model = v_model or "claude-3-5-sonnet-20241022"
    elif provider == "gemini":
        # Gemini via son endpoint compatible OpenAI
        client = OpenAI(
            base_url="https://generativelanguage.googleapis.com/v1beta/openai/",
            api_key=v_key or os.getenv("GEMINI_API_KEY", ""),
        )
        model = v_model or "gemini-2.5-flash-preview-05-20"
    else:
        # Local (LM Studio / Ollama avec modÃ¨le vision â€” ex: llava, minicpm-v)
        client = OpenAI(base_url=v_url, api_key=v_key)
        model  = v_model or config.get("model", "local-model")

    return client, model

def _image_to_base64(file_path: str) -> tuple[str, str]:
    """Encode une image en base64 et retourne (data_url_prefix, b64_string)."""
    ext = os.path.splitext(file_path)[1].lower()
    mime_map = {".jpg": "image/jpeg", ".jpeg": "image/jpeg",
                ".png": "image/png", ".bmp": "image/bmp",
                ".tiff": "image/tiff", ".webp": "image/webp"}
    mime = mime_map.get(ext, "image/jpeg")
    with open(file_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("utf-8")
    return mime, b64


def analyze_with_vision(file_path: str, config: dict) -> dict:
    """
    Analyse un document image via un LLM multimodal (vision).
    Retourne le dict structurÃ© incluant extracted_text.
    """
    logger.info(f"[vision] Analyse vision de : {file_path}")
    try:
        mime, b64 = _image_to_base64(file_path)
        client, model = _get_vision_client(config)

        response = client.chat.completions.create(
            model=model,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "image_url",
                     "image_url": {"url": f"data:{mime};base64,{b64}", "detail": "high"}},
                    {"type": "text", "text": VISION_SYSTEM_PROMPT},
                ],
            }],
            temperature=0.1,
            max_tokens=1500,
        )
        raw = response.choices[0].message.content.strip()
        if raw.startswith("```"):
            raw = raw.split("```")[1]
            if raw.startswith("json"):
                raw = raw[4:]
        result = json.loads(raw)
        logger.info(f"[vision] RÃ©sultat : {result.get('category')} â€” {result.get('summary')}")
        return result
    except json.JSONDecodeError:
        logger.warning("[vision] JSON invalide, fallback analyse texte")
        return {"category": "Autre", "summary": "Analyse vision impossible (JSON invalide)",
                "date": None, "amount": None, "issuer": None, "extracted_text": ""}
    except Exception as e:
        logger.error(f"[vision] Erreur : {e}")
        return {"category": "Erreur", "summary": str(e)[:100],
                "date": None, "amount": None, "issuer": None, "extracted_text": ""}


# ---------------------------------------------------------------------------
# Analyse texte standard par LLM
# ---------------------------------------------------------------------------

def analyze_with_llm(text: str, config: dict | None = None) -> dict:
    """Envoie le texte au LLM et retourne un dict structurÃ©."""
    if config is None:
        config = get_llm_config()
    try:
        client = OpenAI(base_url=config["base_url"], api_key=config["api_key"])
        response = client.chat.completions.create(
            model=config["model"],
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user",   "content": text[:3000]},
            ],
            temperature=0.1,
        )
        raw = response.choices[0].message.content.strip()
        if raw.startswith("```"):
            raw = raw.split("```")[1]
            if raw.startswith("json"):
                raw = raw[4:]
        return json.loads(raw)
    except json.JSONDecodeError:
        return {"category": "Autre", "summary": "Analyse impossible (JSON invalide)",
                "date": None, "amount": None, "issuer": None}
    except Exception as e:
        return {"category": "Erreur", "summary": str(e)[:100],
                "date": None, "amount": None, "issuer": None}


# ---------------------------------------------------------------------------
# Point d'entrÃ©e principal
# ---------------------------------------------------------------------------

def process_document(file_path: str) -> tuple[str, dict]:
    """
    Pipeline complet : OCR â†’ correction LLM â†’ analyse structurÃ©e.
    Si vision activÃ©e et fichier image â†’ bypass OCR, analyse directe par vision.
    Retourne (texte_brut_ou_corrigÃ©, analyse_dict).
    """
    config = get_llm_config()
    ext = os.path.splitext(file_path)[1].lower()
    is_image = ext in (".png", ".jpg", ".jpeg", ".bmp", ".tiff", ".webp")

    # â”€â”€ Chemin vision (image + vision activÃ©e) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if is_image and config.get("vision_enabled"):
        logger.info("[processor] Mode VISION activÃ©")
        vision_result = analyze_with_vision(file_path, config)
        # Le texte extrait par vision sert de contenu OCR stockÃ© en DB
        extracted_text = vision_result.pop("extracted_text", "") or ""
        # Si le texte extrait est substantiel, on l'enrichit avec une passe OCR aussi
        if not extracted_text:
            ocr_text, _ = extract_text_with_confidence(file_path)
            extracted_text = ocr_text
        return extracted_text, vision_result

    # â”€â”€ Chemin OCR + correction LLM + analyse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if is_image:
        ocr_text, confidence = extract_text_with_confidence(file_path)
        logger.info(f"[processor] OCR confiance={confidence:.1f}%")
        # Correction LLM si texte non vide
        if ocr_text.strip():
            # Si un provider vision est configurÃ©, on passe aussi l'image pour meilleure correction
            if config.get("vision_enabled") and config.get("vision_provider"):
                corrected_text = correct_ocr_with_vision(file_path, ocr_text, confidence, config)
            else:
                corrected_text = correct_ocr_with_llm(ocr_text, confidence, config)
        else:
            corrected_text = ocr_text
    else:
        # PDF â€” extraction native, pas d'OCR Tesseract nÃ©cessaire
        corrected_text = _extract_pdf_text(file_path)
        confidence = 100.0

    analysis = analyze_with_llm(corrected_text, config)
    # Stocker le score OCR dans le rÃ©sumÃ© si confiance basse (debug utile)
    if confidence < 60 and is_image:
        analysis["ocr_confidence"] = round(confidence, 1)

    return corrected_text, analysis